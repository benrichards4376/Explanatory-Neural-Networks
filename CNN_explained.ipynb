{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14d45c41-cfae-4cc1-a6fb-d1b1d7b78916",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mazor/nn/nn/lib64/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bebe2da0-22a7-4e40-99e7-d557c7c31722",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"\n",
    "    Set the seed for reproducibility.\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "    np.random.seed(seed)  # Set the seed for numpy for reproducibility\n",
    "    random.seed(seed)  # Set Python random seed\n",
    "    # Ensures that CUDA operations are deterministic\n",
    "    torch.backends.cudnn.deterministic = True  \n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Example: Setting the seed to 42\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb22ac08-6317-4650-83f3-97e3d9c194b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "batch_size = 32\n",
    "# Transform to convert images to PyTorch tensors and normalize them\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# TODO Changing shuffle to True results in the input data shuffling every epoch; \n",
    "# this is causing some undefined behavior with our expected and predicted outputs\n",
    "# Possible fixes: store inputs in a dictionary with unique IDs,\n",
    "# Store trainloader as a class member\n",
    "\n",
    "# Download and load the training data for MNIST\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ab08fb7-e1cc-4540-840a-a97b6d16be5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_images = []\n",
    "all_indices = []\n",
    "for i, (images, labels) in enumerate(trainloader):\n",
    "    all_images.extend(images)\n",
    "    all_indices.extend(range(i * len(images), (i + 1) * len(images)))\n",
    "\n",
    "# Convert the list to a NumPy array\n",
    "all_images = torch.stack(all_images)\n",
    "all_indices = torch.tensor(all_indices)\n",
    "\n",
    "\n",
    "\n",
    "# Function to display an image given its index\n",
    "def imshow_by_index(index):\n",
    "    img = all_images[index] / 2 + 0.5  # Unnormalize\n",
    "    plt.imshow(img.permute(1, 2, 0))  # Convert from Tensor image\n",
    "    plt.show()\n",
    "\n",
    "# Example: Display the first four images\n",
    "for i in range(4):\n",
    "    imshow_by_index(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5adf74a-f735-4b97-b954-47bdc317f41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainableNode(nn.Module):\n",
    "    def __init__(self, input_features, output_features):\n",
    "        super(TrainableNode, self).__init__()\n",
    "        self.linear = nn.Linear(input_features, output_features)\n",
    "        self.is_explainable = True\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "\n",
    "class MNISTNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNISTNet, self).__init__()\n",
    "        # Assuming MNIST images are 1x28x28 (channels x width x height)\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5, stride=1, padding=2) # Output: 32x28x28\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2) # Output: 64x14x14\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2) # Output: 64x7x7 after pooling\n",
    "        \n",
    "        # Flatten: 64*7*7 = 3136 features\n",
    "        self.fc1 = nn.Linear(3136, 128) # Fully connected layer\n",
    "        \n",
    "        # Trainable node for explainability, assuming it requires a flattened input\n",
    "        self.explainable_node = TrainableNode(128, 1)\n",
    "        \n",
    "        self.fc2 = nn.Linear(128, 64) # Additional fully connected layer\n",
    "        self.fc3 = nn.Linear(64, 10) # Final layer for classifying into 10 classes\n",
    "        \n",
    "    def forward(self, x, return_explainable=False):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        # Flatten the output for the fully connected layers\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        x1 = F.relu(self.fc1(x))\n",
    "        \n",
    "        if return_explainable:\n",
    "            explainable_output = self.explainable_node(x1).squeeze()\n",
    "        \n",
    "        x = F.relu(self.fc2(x1))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        if return_explainable:\n",
    "            return x, explainable_output\n",
    "        return x\n",
    "\n",
    "\n",
    "    def forward_with_explainable(self, x):\n",
    "        # Directly pass the input to the explainable_node without additional transformations\n",
    "        explainable_output = self.explainable_node(x)\n",
    "        return explainable_output\n",
    "        \n",
    "\n",
    "class ExplainableNetwork:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.activations_per_input = {}\n",
    "        self.gradients_per_input = {} \n",
    "        self.current_epoch = 0\n",
    "        self.num_epochs = 0\n",
    "        self.batch_counter = 0\n",
    "        self.layer_names = []\n",
    "        self.explainable_module_names = set()\n",
    "        explainable_module_names = set()\n",
    "        for name, module in model.named_modules():\n",
    "            if getattr(module, 'is_explainable', False):\n",
    "                explainable_module_names.add(name)\n",
    "        for name, module in model.named_modules():\n",
    "            if name and not any(part in explainable_module_names for part in name.split('.')):\n",
    "                self.layer_names.append(name)\n",
    "            elif name:\n",
    "                self.explainable_module_names.add(name)\n",
    "\n",
    "        self.expected_outputs = []\n",
    "        self.predicted_outputs = []\n",
    "\n",
    "        self.hook_handles = [] \n",
    "\n",
    "        self.enable_hooks()\n",
    "        \n",
    "    # def getNonExpLayers(model):\n",
    "    #     layers = []\n",
    "    #     for name, module in model.named_modules():\n",
    "    #         if name != '' and not isinstance(module, TrainableNode):\n",
    "    #             layers.append(name)\n",
    "    #     return layers\n",
    "\n",
    "    # Register hooks for each layer\n",
    "    def enable_hooks(self):\n",
    "        for name, module in self.model.named_modules():\n",
    "            if name in self.explainable_module_names:  # Skip the explainable_node by its name\n",
    "                continue\n",
    "            # Check if the module is a type that we want to hook (Conv2d, Linear, or pooling layers)\n",
    "            if isinstance(module, (torch.nn.Linear, torch.nn.Conv2d, torch.nn.MaxPool2d)):\n",
    "                forward_handle = module.register_forward_hook(self.create_forward_hook(name))\n",
    "                backward_handle = module.register_full_backward_hook(self.create_backward_hook(name))\n",
    "                self.hook_handles.append(forward_handle)\n",
    "                self.hook_handles.append(backward_handle)\n",
    "    def disable_hooks(self):\n",
    "        for handle in self.hook_handles:\n",
    "            handle.remove()\n",
    "        self.hook_handles = []  # Clear the list after removing all hooks\n",
    "\n",
    "    def create_forward_hook(self, layer_name):\n",
    "        def forward_hook(module, input, output):\n",
    "            if self.current_epoch == self.num_epochs - 1:\n",
    "                self.store_activations(layer_name, output)\n",
    "        return forward_hook\n",
    "\n",
    "    def create_backward_hook(self, layer_name):\n",
    "        def backward_hook(module, grad_input, grad_output):\n",
    "            if self.current_epoch == self.num_epochs - 1:\n",
    "                self.store_gradients(layer_name, grad_output[0])\n",
    "        return backward_hook\n",
    "    def store_activations(self, layer_name, output):\n",
    "        self._store_per_input(self.activations_per_input, layer_name, output)\n",
    "\n",
    "    def store_gradients(self, layer_name, gradient):\n",
    "        # Store gradients for the last epoch\n",
    "        if self.current_epoch == self.num_epochs - 1:\n",
    "            batch_size = gradient.size(0)\n",
    "            for i in range(batch_size):\n",
    "                input_index = self.batch_counter * batch_size + i\n",
    "                if input_index not in self.gradients_per_input:\n",
    "                    self.gradients_per_input[input_index] = {}\n",
    "                self.gradients_per_input[input_index][layer_name] = gradient[i].detach().cpu().numpy()\n",
    "    \n",
    "            # Increment the backward batch counter only after processing the first layer (which is the last layer in the forward pass)\n",
    "\n",
    "\n",
    "    def _store_per_input(self, storage_dict, layer_name, tensor):\n",
    "        batch_size = tensor.size(0)\n",
    "        for i in range(batch_size):\n",
    "            input_index = self.batch_counter * batch_size + i\n",
    "            if input_index not in storage_dict:\n",
    "                storage_dict[input_index] = {}\n",
    "            storage_dict[input_index][layer_name] = tensor[i].detach().cpu().numpy()\n",
    "\n",
    "        if layer_name == self.layer_names[-1]:\n",
    "            self.batch_counter += 1\n",
    "\n",
    "    def start_epoch(self, epoch, num_epochs):\n",
    "        self.current_epoch = epoch\n",
    "        self.num_epochs = num_epochs\n",
    "        if epoch == num_epochs - 1:\n",
    "            self.batch_counter = 0  # Reset the batch counter on the last epoch\n",
    "\n",
    "    def get_activations(self, input_index):\n",
    "        return self.activations_per_input.get(input_index, None)\n",
    "\n",
    "    def get_gradients(self, input_index):\n",
    "        return self.gradients_per_input.get(input_index, None)\n",
    "\n",
    "    def print_structure(self):\n",
    "        print(\"Explainable Network Structure and Data Summary:\")\n",
    "\n",
    "        # Check if there is any data collected\n",
    "        if not self.activations_per_input:\n",
    "            print(\"No activations data collected.\")\n",
    "            return\n",
    "\n",
    "        if not self.gradients_per_input:\n",
    "            print(\"No gradients data collected.\")\n",
    "            return\n",
    "\n",
    "        # Print information about activations\n",
    "        print(\"\\nActivations:\")\n",
    "        first_input_activations = next(iter(self.activations_per_input.values()))\n",
    "        for layer_name, activations in first_input_activations.items():\n",
    "            num_nodes = len(activations)\n",
    "            num_data_points = len(self.activations_per_input)\n",
    "            print(f\"Layer {layer_name}: {num_nodes} nodes, {num_data_points} data points each\")\n",
    "\n",
    "        # Print information about gradients\n",
    "        print(\"\\nGradients:\")\n",
    "        first_input_gradients = next(iter(self.gradients_per_input.values()))\n",
    "        for layer_name, gradients in first_input_gradients.items():\n",
    "            num_nodes = len(gradients)\n",
    "            num_data_points = len(self.gradients_per_input)\n",
    "            print(f\"Layer {layer_name}: {num_nodes} nodes, {num_data_points} data points each\")\n",
    "            \n",
    "    # FIXME: Add functionality for first layer\n",
    "    def get_node_input(self, layer_name, node_index, image_index):\n",
    "        if layer_name != self.layer_names[0] and layer_name in self.layer_names:\n",
    "            # For subsequent layers, we use activations from the previous layer\n",
    "            prev_layer_name = self.get_previous_layer_name(layer_name)\n",
    "            prev_activations = self.activations_per_input[image_index][prev_layer_name]\n",
    "        \n",
    "            # Ensure the prev_activations is a 1D array for proper multiplication\n",
    "            if prev_activations.ndim > 1:\n",
    "                prev_activations = prev_activations.flatten()\n",
    "\n",
    "        return prev_activations\n",
    "\n",
    "    def find_extreme_activations(self, std_dev_threshold=3, label_digit=0, pred_digit=0):\n",
    "            \"\"\"\n",
    "            Find inputs where node activations are more than +3 or less than -3 standard deviations from the mean.\n",
    "\n",
    "            Args:\n",
    "            std_dev_threshold (float): The number of standard deviations to use as a threshold for extreme activations.\n",
    "\n",
    "            Returns:\n",
    "            dict: A dictionary containing two keys ('high' and 'low') each mapping to a dict of nodes and their corresponding extreme input indices.\n",
    "            \"\"\"\n",
    "\n",
    "            # Calculate mean and standard deviation for each node\n",
    "            node_stats = {}  # Dictionary to store mean and std for each node\n",
    "            for _, layers_activations in self.activations_per_input.items():\n",
    "                for layer_name, activations in layers_activations.items():\n",
    "                    if layer_name not in node_stats:\n",
    "                        node_stats[layer_name] = []\n",
    "                    node_stats[layer_name].append(activations)\n",
    "\n",
    "            for layer_name, activations_list in node_stats.items():\n",
    "                node_stats[layer_name] = {\n",
    "                    'mean': np.mean(activations_list, axis=0),\n",
    "                    'std': np.std(activations_list, axis=0)\n",
    "                }\n",
    "\n",
    "            activations_structure = {}\n",
    "\n",
    "            # Assuming 'model' is your neural network model\n",
    "            for name, module in self.model.named_modules():\n",
    "                # Skip the model itself as it's also returned by named_modules\n",
    "                if name != \"\" and name in self.layer_names:\n",
    "                    activations_structure[name] = {'high': {}, 'low': {}}\n",
    "\n",
    "            # Identify inputs with extreme activations\n",
    "            for input_index, layers_activations in self.activations_per_input.items():\n",
    "                for layer_name, activations in layers_activations.items():\n",
    "                    if self.expected_outputs[input_index] == label_digit and self.predicted_outputs[input_index] == pred_digit:\n",
    "                        mean = node_stats[layer_name]['mean']\n",
    "                        std = node_stats[layer_name]['std']\n",
    "                        high_threshold = mean + std_dev_threshold * std\n",
    "                        low_threshold = mean - std_dev_threshold * std\n",
    "\n",
    "                        high_indices = np.where(activations > high_threshold)[0]\n",
    "                        low_indices = np.where(activations < low_threshold)[0]\n",
    "\n",
    "                        for idx in high_indices:\n",
    "                            activations_structure[layer_name]['high'][idx] = activations_structure[layer_name]['high'].get(idx, 0) + 1\n",
    "                        for idx in low_indices:\n",
    "                            activations_structure[layer_name]['low'][idx] = activations_structure[layer_name]['low'].get(idx, 0) + 1\n",
    "            return activations_structure\n",
    "            \n",
    "    def get_previous_layer_name(self, current_layer_name):\n",
    "        try:\n",
    "            index = self.layer_names.index(current_layer_name)\n",
    "        except ValueError:\n",
    "            print(\"Layer name not found in the model.\")\n",
    "            return None\n",
    "        \n",
    "        if index == 0:\n",
    "            return None\n",
    "        else:\n",
    "            return self.layer_names[index - 1]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41d90918-8c94-4e92-b15a-fd2ab33feff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers: ['conv1', 'conv2', 'pool', 'fc1', 'fc2', 'fc3']\n",
      "explainable layers: {'explainable_node', 'explainable_node.linear'}\n"
     ]
    }
   ],
   "source": [
    "model = MNISTNet().to(device)\n",
    "explainable_net = ExplainableNetwork(model)\n",
    "print(f'layers: {explainable_net.layer_names}')\n",
    "print(f'explainable layers: {explainable_net.explainable_module_names}')\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 2  # Adjust as needed\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    explainable_net.start_epoch(epoch, num_epochs)\n",
    "    model.train()\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = F.cross_entropy(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #scheduler.step()\n",
    "        if epoch == num_epochs - 1:\n",
    "            explainable_net.expected_outputs.extend(targets.cpu().numpy())\n",
    "            \n",
    "            # Convert outputs to predicted class labels\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            explainable_net.predicted_outputs.extend(predicted.cpu().numpy())\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72f533a9-4e95-463b-b1a9-9982fc7dbc3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explainable Network Structure and Data Summary:\n",
      "\n",
      "Activations:\n",
      "Layer conv1: 32 nodes, 60000 data points each\n",
      "Layer pool: 64 nodes, 60000 data points each\n",
      "Layer conv2: 64 nodes, 60000 data points each\n",
      "Layer fc1: 128 nodes, 60000 data points each\n",
      "Layer fc2: 64 nodes, 60000 data points each\n",
      "Layer fc3: 10 nodes, 60000 data points each\n",
      "\n",
      "Gradients:\n",
      "Layer fc3: 10 nodes, 60000 data points each\n",
      "Layer fc2: 64 nodes, 60000 data points each\n",
      "Layer fc1: 128 nodes, 60000 data points each\n",
      "Layer pool: 32 nodes, 60000 data points each\n",
      "Layer conv2: 64 nodes, 60000 data points each\n",
      "Layer conv1: 32 nodes, 60000 data points each\n"
     ]
    }
   ],
   "source": [
    "explainable_net.print_structure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "174c81d3-8fed-407e-9089-5b6d3fc2379b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "60000\n",
      "Sample of Expected Outputs: [5, 0, 4, 1, 9, 2, 1, 3, 1, 4]\n",
      "Sample of Predicted Outputs: [5, 0, 4, 1, 9, 2, 1, 3, 1, 4]\n"
     ]
    }
   ],
   "source": [
    "sample_size = 10\n",
    "\n",
    "print(len(explainable_net.expected_outputs))\n",
    "print(len(explainable_net.predicted_outputs))\n",
    "\n",
    "print(\"Sample of Expected Outputs:\", explainable_net.expected_outputs[:sample_size])\n",
    "print(\"Sample of Predicted Outputs:\", explainable_net.predicted_outputs[:sample_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c7ada70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizes the predictions (i, j) to the total number of predictions(i, _) for all predictions\n",
    "# Returns a 2D array such that probabilities[i][j] holds the probability of the model\n",
    "# predicting last_layer[i] as last_layer[j]\n",
    "def calculate_probabilities(classifier_outcomes, label_counts):\n",
    "    probabilities = np.zeros_like(classifier_outcomes, dtype=np.float64)\n",
    "    for i in range(len(label_counts)):\n",
    "        if label_counts[i] > 0:  # Prevent division by zero\n",
    "            probabilities[i] = classifier_outcomes[i] / label_counts[i]\n",
    "    return probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ddf98c49-2029-4c06-b58c-3abe1962cb36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the 10000 test images: 98.87%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "final_layer_size = list(explainable_net.model.named_modules())[-1][1].out_features\n",
    "# classifier_outcomes[i][j] will store the probability of the model incorrectly classifying digit i as digit j.\n",
    "# The diagonals store the probability of a correct predicition\n",
    "classifier_outcomes = np.zeros((final_layer_size, final_layer_size), dtype=np.float64)\n",
    "label_counts = np.zeros(final_layer_size, dtype=np.int64)\n",
    "\n",
    "explainable_net.disable_hooks() \n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Initialize classifier_outcomes as the frequency of each prediction (label, pred)\n",
    "        for label, pred in zip(labels, predicted):\n",
    "            classifier_outcomes[label.item()][pred.item()] += 1\n",
    "            # Store the number of total attempts at predicting 'label'\n",
    "            label_counts[label.item()] += 1\n",
    "\n",
    "classifier_outcomes = calculate_probabilities(classifier_outcomes, label_counts)\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Accuracy of the model on the 10000 test images: {accuracy}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6343fcc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Predicted 0  Predicted 1  Predicted 2  Predicted 3  Predicted 4  \\\n",
      "Actual 0       0.9959       0.0010       0.0000       0.0000       0.0000   \n",
      "Actual 1       0.0000       0.9974       0.0009       0.0000       0.0000   \n",
      "Actual 2       0.0010       0.0019       0.9922       0.0000       0.0000   \n",
      "Actual 3       0.0000       0.0000       0.0040       0.9921       0.0000   \n",
      "Actual 4       0.0000       0.0000       0.0010       0.0000       0.9969   \n",
      "Actual 5       0.0011       0.0000       0.0011       0.0056       0.0000   \n",
      "Actual 6       0.0042       0.0042       0.0000       0.0000       0.0010   \n",
      "Actual 7       0.0000       0.0029       0.0097       0.0000       0.0000   \n",
      "Actual 8       0.0031       0.0000       0.0031       0.0031       0.0010   \n",
      "Actual 9       0.0010       0.0020       0.0000       0.0010       0.0059   \n",
      "\n",
      "          Predicted 5  Predicted 6  Predicted 7  Predicted 8  Predicted 9  \n",
      "Actual 0       0.0000       0.0000       0.0000       0.0020       0.0010  \n",
      "Actual 1       0.0018       0.0000       0.0000       0.0000       0.0000  \n",
      "Actual 2       0.0000       0.0000       0.0039       0.0010       0.0000  \n",
      "Actual 3       0.0020       0.0000       0.0010       0.0010       0.0000  \n",
      "Actual 4       0.0000       0.0000       0.0000       0.0010       0.0010  \n",
      "Actual 5       0.9899       0.0000       0.0011       0.0000       0.0011  \n",
      "Actual 6       0.0115       0.9791       0.0000       0.0000       0.0000  \n",
      "Actual 7       0.0000       0.0000       0.9835       0.0010       0.0029  \n",
      "Actual 8       0.0031       0.0000       0.0021       0.9815       0.0031  \n",
      "Actual 9       0.0040       0.0000       0.0059       0.0030       0.9772  \n"
     ]
    }
   ],
   "source": [
    "# Turns classifier_outcomes into a pandas.DataFrame for easier visualization\n",
    "def display_classifier_outcomes_table(classifier_outcomes, mode='percent'):\n",
    "    size = classifier_outcomes.shape[0]\n",
    "    df = pd.DataFrame(classifier_outcomes, \n",
    "                      columns=[f'Predicted {j}' for j in range(size)], \n",
    "                      index=[f'Actual {i}' for i in range(size)])\n",
    "    \n",
    "    if mode == 'percent':\n",
    "        df = df * 100\n",
    "        fmt = \"{:.4f}%\"\n",
    "    elif mode == 'probability':\n",
    "        fmt = \"{:.4f}\"\n",
    "    \n",
    "    pd.set_option('display.float_format', fmt.format)\n",
    "    \n",
    "    # Display table\n",
    "    print(df)\n",
    "\n",
    "display_classifier_outcomes_table(classifier_outcomes, 'probability')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9abbe6e-82d7-4abf-9483-b8132ce98a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_digit = 0\n",
    "pred_digit = 0\n",
    "#extreme_activations_result = explainable_net.find_extreme_activations(label_digit=label_digit, pred_digit=pred_digit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7a67518-c07e-4e8b-a62f-94a3e4f45e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_activation_structure(struct, layer_names):\n",
    "    extremity_types = ['high', 'low']\n",
    "\n",
    "    for layer_name in layer_names:\n",
    "        print(f\"  Layer: {layer_name}\")\n",
    "        for ext_type in extremity_types:\n",
    "            # Access the dictionary for the current layer and extremity type\n",
    "            current_dict = struct[layer_name][ext_type]\n",
    "            # Check if there are any nodes\n",
    "            if current_dict:\n",
    "                # Prepare the formatted string of (key, value) pairs\n",
    "                formatted_pairs = ', '.join([f\"({key}, {value})\" for key, value in current_dict.items()])\n",
    "                print(f\"    {'High' if ext_type == 'high' else 'Low'} Extreme Activations: Nodes [{formatted_pairs}]\")\n",
    "            else:\n",
    "                print(f\"    {ext_type} Extreme Activations: None\")\n",
    "        print()  # New line for better readability between layers\n",
    "\n",
    "\n",
    "# # Example usage\n",
    "#print_activation_structure(extreme_activations_result, explainable_net.layer_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e90161f6-07ad-4c30-8329-28fc98d06ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_name = 'pool'\n",
    "node_index = 1\n",
    "std_dev = 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f908b789-33ba-4f08-b71c-54e2df89ec72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def plot_extreme_outliers(explainable_network, layer_name, node_index, std_dev_threshold=3, discriminate_classes=[-1,-1], Correct_pred_only=False):\n",
    "    activations = []\n",
    "    gradients = []\n",
    "    colors = []\n",
    "    labels = []\n",
    "    classA = discriminate_classes[0]\n",
    "    classB = discriminate_classes[1]\n",
    "    \n",
    "    # Define a color map for digits 0-9\n",
    "    color_map = matplotlib.colormaps['tab10']\n",
    "\n",
    "    # Retrieve activations, gradients, and check for correct predictions\n",
    "    for input_index, (expected, predicted) in enumerate(zip(explainable_network.expected_outputs, explainable_network.predicted_outputs)):\n",
    "        if expected == predicted or not Correct_pred_only:\n",
    "            activation = explainable_network.get_activations(input_index)\n",
    "            gradient = explainable_network.get_gradients(input_index)\n",
    "\n",
    "            if activation is not None and gradient is not None:\n",
    "                activations.append(activation[layer_name][node_index])\n",
    "                gradients.append(-gradient[layer_name][node_index])  # Negative of the gradient\n",
    "                colors.append(color_map(expected))  # Color code based on the correct digit\n",
    "                labels.append(expected)\n",
    "            \n",
    "\n",
    "    # Convert lists to numpy arrays for numerical operations\n",
    "    activations = np.array(activations)\n",
    "    gradients = np.array(gradients)\n",
    "    labels = np.array(labels)\n",
    "    colors = np.array(colors)\n",
    "\n",
    "    # Calculate mean and standard deviation for activations and gradients\n",
    "    activation_mean = np.mean(activations)\n",
    "    activation_std = np.std(activations)\n",
    "    gradient_mean = np.mean(gradients)\n",
    "    gradient_std = np.std(gradients)\n",
    "\n",
    "    # Apply filtering for points that are more than std_dev_threshold away in both dimensions\n",
    "    extreme_outliers = (np.abs(activations - activation_mean) > std_dev_threshold * activation_std) & \\\n",
    "                       (np.abs(gradients - gradient_mean) > std_dev_threshold * gradient_std)\n",
    "\n",
    "    # Filter activations, gradients, and colors based on the extreme outlier condition\n",
    "    activations_extreme = activations[extreme_outliers]\n",
    "    gradients_extreme = gradients[extreme_outliers]\n",
    "    labels_extreme = labels[extreme_outliers]\n",
    "    colors_extreme = colors[extreme_outliers]\n",
    "\n",
    "    # Create Dataframe for easy use\n",
    "    df = pd.DataFrame({'Activations': activations_extreme, 'Gradients': gradients_extreme, 'Labels': labels_extreme})\n",
    "    \n",
    "    if classA > -1 and classB > -1:\n",
    "        #condition = np.logical_or(colors_extreme[labels == classA], colors_extreme[labels == classB])\n",
    "        colors_extreme = colors_extreme[df['Labels'].isin([classA, classB])]\n",
    "        df = df[df['Labels'].isin([classA, classB])]\n",
    "\n",
    "    # Generate scatter plot with extreme outliers\n",
    "    fig, ax = plt.subplots(figsize=(15, 10))\n",
    "    scatter = ax.scatter(df.loc[:, ['Activations']], df.loc[:, ['Gradients']], c= colors_extreme, cmap=color_map, s=15)\n",
    "    ax.set_xlabel('Activation')\n",
    "    ax.set_ylabel('Negative of Gradient')\n",
    "    ax.set_title(f'Extreme Outliers Scatter Plot for {layer_name} - Node {node_index}')\n",
    "\n",
    "    # Create a colorbar\n",
    "    cbar = plt.colorbar(matplotlib.cm.ScalarMappable(cmap=color_map, norm=plt.Normalize(0, 9)), ax=ax, ticks=np.arange(0, 10))\n",
    "    cbar.set_label('Digit')\n",
    "\n",
    "    ax.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "70b06f04-9003-4dd9-8462-1c69c68f5333",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (59968,7,7) (59968,14,14) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mplot_extreme_outliers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexplainable_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd_dev_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCorrect_pred_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Adjust std_dev_threshold as needed\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[21], line 40\u001b[0m, in \u001b[0;36mplot_extreme_outliers\u001b[0;34m(explainable_network, layer_name, node_index, std_dev_threshold, discriminate_classes, Correct_pred_only)\u001b[0m\n\u001b[1;32m     37\u001b[0m gradient_std \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstd(gradients)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Apply filtering for points that are more than std_dev_threshold away in both dimensions\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m extreme_outliers \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactivations\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mactivation_mean\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstd_dev_threshold\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mactivation_std\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m&\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m                   \u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgradients\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mgradient_mean\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstd_dev_threshold\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mgradient_std\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Filter activations, gradients, and colors based on the extreme outlier condition\u001b[39;00m\n\u001b[1;32m     44\u001b[0m activations_extreme \u001b[38;5;241m=\u001b[39m activations[extreme_outliers]\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (59968,7,7) (59968,14,14) "
     ]
    }
   ],
   "source": [
    "plot_extreme_outliers(explainable_net, layer_name, node_index, std_dev_threshold=0, Correct_pred_only=False)  # Adjust std_dev_threshold as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42430301-d556-4c83-b8ec-f8b17159469d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_extreme_outliers(explainable_net, layer_name, node_index, std_dev_threshold=std_dev, Correct_pred_only=False)  # Adjust std_dev_threshold as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc05065-b44b-43b7-819a-1db25807d8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_extreme_outliers(explainable_net, layer_name, node_index, std_dev_threshold=std_dev, Correct_pred_only=False, discriminate_classes=[1,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce092ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def plot_extreme_outliers_quad(explainable_network, layer_name, node_index, quadrant, std_dev_threshold=3, selected_class=-1):\n",
    "    activations = []\n",
    "    gradients = []\n",
    "    labels = []\n",
    "    imgIndx = []\n",
    "    colors = []\n",
    "    \n",
    "     # Define a color map for digits 0-9\n",
    "    color_map = matplotlib.colormaps['tab10']\n",
    "\n",
    "    # Retrieve activations, gradients, and check for correct predictions\n",
    "    for input_index, (expected, predicted) in enumerate(zip(explainable_network.expected_outputs, explainable_network.predicted_outputs)):\n",
    "        #if expected == predicted: do we want to filter this?\n",
    "        activation = explainable_network.get_activations(input_index)\n",
    "        gradient = explainable_network.get_gradients(input_index)\n",
    "\n",
    "        if activation is not None and gradient is not None:\n",
    "            activations.append(activation[layer_name][node_index])\n",
    "            gradients.append(-gradient[layer_name][node_index])  # Negative of the gradient\n",
    "            labels.append(expected)\n",
    "            imgIndx.append(input_index)\n",
    "            colors.append(color_map(expected))  # Color code based on the correct digit\n",
    "            \n",
    "\n",
    "    # Convert lists to numpy arrays for numerical operations\n",
    "    activations = np.array(activations)\n",
    "    gradients = np.array(gradients)\n",
    "    labels = np.array(labels)\n",
    "    imgIndx = np.array(imgIndx)\n",
    "    colors = np.array(colors)\n",
    "\n",
    "\n",
    "    # Calculate mean and standard deviation for activations and gradients\n",
    "    activation_mean = np.mean(activations)\n",
    "    activation_std = np.std(activations)\n",
    "    gradient_mean = np.mean(gradients)\n",
    "    gradient_std = np.std(gradients)\n",
    "\n",
    "    # Apply filtering for points that are more than std_dev_threshold away in both dimensions\n",
    "    extreme_outliers = (np.abs(activations - activation_mean) > std_dev_threshold * activation_std) & \\\n",
    "                       (np.abs(gradients - gradient_mean) > std_dev_threshold * gradient_std)\n",
    "\n",
    "    # Filter activations, gradients, and colors based on the extreme outlier condition\n",
    "    activations_extreme = activations[extreme_outliers]\n",
    "    gradients_extreme = gradients[extreme_outliers]\n",
    "    labels_extreme = labels[extreme_outliers]\n",
    "    colors_extreme = colors[extreme_outliers]\n",
    "    imgIndx = imgIndx[extreme_outliers]\n",
    "\n",
    "    # Create Dataframe for easy use\n",
    "    df = pd.DataFrame({'Index':imgIndx,'Activations': activations_extreme, 'Gradients': gradients_extreme, 'Labels': labels_extreme})\n",
    "    \n",
    "    if selected_class > -1:\n",
    "        colors_extreme = colors_extreme[df['Labels'] == selected_class]\n",
    "        df = df[df['Labels'] == selected_class]\n",
    "        \n",
    "    if quadrant == 1:\n",
    "        colors_extreme = colors_extreme[df['Gradients'] > 0]\n",
    "        df = df[df['Gradients'] > 0]\n",
    "        colors_extreme = colors_extreme[df['Activations'] > 0]\n",
    "        df = df[df['Activations'] > 0]\n",
    "    elif quadrant == 2:\n",
    "        colors_extreme = colors_extreme[df['Gradients'] > 0]\n",
    "        df = df[df['Gradients'] > 0]\n",
    "        colors_extreme = colors_extreme[df['Activations'] < 0]\n",
    "        df = df[df['Activations'] < 0]\n",
    "        \n",
    "    elif quadrant == 3:\n",
    "        colors_extreme = colors_extreme[df['Gradients'] < 0]\n",
    "        df = df[df['Gradients'] < 0]\n",
    "        colors_extreme = colors_extreme[df['Activations'] < 0]\n",
    "        df = df[df['Activations'] < 0]\n",
    "        \n",
    "    elif quadrant == 4:\n",
    "        colors_extreme = colors_extreme[df['Gradients'] < 0]\n",
    "        df = df[df['Gradients'] < 0]\n",
    "        colors_extreme = colors_extreme[df['Activations'] > 0]\n",
    "        df = df[df['Activations'] > 0]\n",
    "        \n",
    "    # Generate scatter plot with extreme outliers\n",
    "    fig, ax = plt.subplots(figsize=(15, 10))\n",
    "    scatter = ax.scatter(df.loc[:, ['Activations']], df.loc[:, ['Gradients']], c=colors_extreme, cmap=color_map, s=15)\n",
    "    ax.set_xlabel('Activation')\n",
    "    ax.set_ylabel('Negative of Gradient')\n",
    "    ax.set_title(f'Extreme Outliers Scatter Plot for {layer_name} - Node {node_index}')\n",
    "\n",
    "    # Create a colorbar\n",
    "    cbar = plt.colorbar(matplotlib.cm.ScalarMappable(cmap=color_map, norm=plt.Normalize(0, 9)), ax=ax, ticks=np.arange(0, 10))\n",
    "    cbar.set_label('Digit')\n",
    "\n",
    "    ax.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "plot_extreme_outliers_quad(explainable_net, layer_name, node_index, std_dev_threshold=1.5, quadrant= 1, selected_class=1)  # Adjust std_dev_threshold as needed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53d1820",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def extreme_outliers_quad(explainable_network, layer_name, node_index, quadrant, std_dev_threshold=3, selected_class=-1):\n",
    "    activations = []\n",
    "    gradients = []\n",
    "    labels = []\n",
    "    imgIndx = []\n",
    "\n",
    "    # Retrieve activations, gradients, and check for correct predictions\n",
    "    for input_index, (expected, predicted) in enumerate(zip(explainable_network.expected_outputs, explainable_network.predicted_outputs)):\n",
    "        #if expected == predicted: do we want to filter this?\n",
    "        activation = explainable_network.get_activations(input_index)\n",
    "        gradient = explainable_network.get_gradients(input_index)\n",
    "\n",
    "        if activation is not None and gradient is not None:\n",
    "            activations.append(activation[layer_name][node_index])\n",
    "            gradients.append(-gradient[layer_name][node_index])  # Negative of the gradient\n",
    "            labels.append(expected)\n",
    "            imgIndx.append(input_index)\n",
    "            \n",
    "\n",
    "    # Convert lists to numpy arrays for numerical operations\n",
    "    activations = np.array(activations)\n",
    "    gradients = np.array(gradients)\n",
    "    labels = np.array(labels)\n",
    "    imgIndx = np.array(imgIndx)\n",
    "\n",
    "\n",
    "    # Calculate mean and standard deviation for activations and gradients\n",
    "    activation_mean = np.mean(activations)\n",
    "    activation_std = np.std(activations)\n",
    "    gradient_mean = np.mean(gradients)\n",
    "    gradient_std = np.std(gradients)\n",
    "\n",
    "    # Apply filtering for points that are more than std_dev_threshold away in both dimensions\n",
    "    extreme_outliers = (np.abs(activations - activation_mean) > std_dev_threshold * activation_std) & \\\n",
    "                       (np.abs(gradients - gradient_mean) > std_dev_threshold * gradient_std)\n",
    "\n",
    "    # Filter activations, gradients, and colors based on the extreme outlier condition\n",
    "    activations_extreme = activations[extreme_outliers]\n",
    "    gradients_extreme = gradients[extreme_outliers]\n",
    "    labels_extreme = labels[extreme_outliers]\n",
    "    imgIndx = imgIndx[extreme_outliers]\n",
    "\n",
    "    # Create Dataframe for easy use\n",
    "    df = pd.DataFrame({'Index':imgIndx,'Activations': activations_extreme, 'Gradients': gradients_extreme, 'Labels': labels_extreme})\n",
    "    \n",
    "    if selected_class > -1:\n",
    "        df = df[df['Labels'] == selected_class]\n",
    "        \n",
    "    if quadrant == 1:\n",
    "        df = df[df['Activations'] > 0]\n",
    "        df = df[df['Gradients'] > 0]\n",
    "        \n",
    "    elif quadrant == 2:\n",
    "        df = df[df['Activations'] < 0]\n",
    "        df = df[df['Gradients'] > 0]\n",
    "        \n",
    "    elif quadrant == 3:\n",
    "        df = df[df['Activations'] < 0]\n",
    "        df = df[df['Gradients'] < 0]\n",
    "        \n",
    "    elif quadrant == 4:\n",
    "        df = df[df['Activations'] > 0]\n",
    "        df = df[df['Gradients'] < 0]\n",
    "    \n",
    "    return df['Index'].values\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "displayPos = extreme_outliers_quad(explainable_net, 'fc2', node_index, std_dev_threshold=1.5, quadrant= 1)  # Adjust std_dev_threshold as needed\n",
    "displayNeg = extreme_outliers_quad(explainable_net, 'fc2', node_index, std_dev_threshold=1.5, quadrant= 3)  # Adjust std_dev_threshold as needed\n",
    "\n",
    "truePos =  extreme_outliers_quad(explainable_net, 'fc2', node_index, std_dev_threshold=1.5, quadrant= 1, selected_class = 1)  \n",
    "trueNeg =  extreme_outliers_quad(explainable_net, 'fc2', node_index, std_dev_threshold=1.5, quadrant= 3, selected_class = 3)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f64074-ed5a-4496-bdd2-1586f7ec349b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image_grid(images, rows, cols):\n",
    "    grid = torchvision.utils.make_grid(images, nrow=cols)\n",
    "    grid = grid / 2 + 0.5  # Unnormalize\n",
    "    plt.figure(figsize=(15, 15))  # Adjust the size as needed\n",
    "    plt.imshow(grid.permute(1, 2, 0))\n",
    "    plt.show()\n",
    "\n",
    "# Number of images per grid\n",
    "images_per_grid = 128  # for example, a 4x4 grid\n",
    "\n",
    "# Calculate rows and columns for the grid\n",
    "cols = 10  # Number of columns\n",
    "rows = images_per_grid // cols\n",
    "\n",
    "# Display the images in grids\n",
    "for i in range(0, len(displayPos), images_per_grid):\n",
    "    selected_indices = displayPos[i:i+images_per_grid]\n",
    "    selected_images = torch.stack([all_images[index] for index in selected_indices])\n",
    "    display_image_grid(selected_images, rows, cols)\n",
    "\n",
    "for i in range(0, len(displayNeg), images_per_grid):\n",
    "    selected_indices = displayNeg[i:i+images_per_grid]\n",
    "    selected_images = torch.stack([all_images[index] for index in selected_indices])\n",
    "    display_image_grid(selected_images, rows, cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018d6027-abd4-4bb2-ae6e-087896b2f2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_subset = torch.stack([all_images[i] for i in displayPos])\n",
    "average_image = images_subset.float().mean(dim=0).squeeze()\n",
    "plt.imshow(average_image, cmap='gray')\n",
    "plt.title(\"Average Positive Image\")\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "normalized_image = (average_image - average_image.min()) / (average_image.max() - average_image.min())\n",
    "\n",
    "# Apply a power law transformation to increase contrast, emphasizing the bright pixels\n",
    "# Squaring the normalized pixel values will make high values brighter and low values dimmer\n",
    "emphasized_positive_image = normalized_image ** 4\n",
    "\n",
    "# Plot the emphasized average image\n",
    "plt.imshow(emphasized_positive_image, cmap='gray')\n",
    "plt.title(\"Emphasized Positive Average Image\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f7a7bc-0f0e-4b7f-a9c2-588836884e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_subset = torch.stack([all_images[i] for i in displayNeg])\n",
    "average_image = images_subset.float().mean(dim=0).squeeze()\n",
    "plt.imshow(average_image, cmap='gray')\n",
    "plt.title(\"Average Negative Image\")\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "normalized_image = (average_image - average_image.min()) / (average_image.max() - average_image.min())\n",
    "\n",
    "# Apply a power law transformation to increase contrast, emphasizing the bright pixels\n",
    "# Squaring the normalized pixel values will make high values brighter and low values dimmer\n",
    "emphasized_negative_image = normalized_image ** 4\n",
    "\n",
    "# Plot the emphasized average image\n",
    "plt.imshow(emphasized_negative_image, cmap='gray')\n",
    "plt.title(\"Emphasized Average Negative Image\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348c6852-f579-4087-90f1-b4a829866599",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# TODO do some math to average pos and neg not just > or <\n",
    "\n",
    "# Create an empty RGB image\n",
    "heatmap = np.zeros((*emphasized_positive_image.shape, 3))\n",
    "\n",
    "# Define a threshold for neutrality\n",
    "threshold = 0.05  # Adjust based on your data's scale\n",
    "\n",
    "# Fill the heatmap\n",
    "for i in range(heatmap.shape[0]):\n",
    "    for j in range(heatmap.shape[1]):\n",
    "        pos = emphasized_positive_image[i, j]\n",
    "        neg = emphasized_negative_image[i, j]\n",
    "        if np.abs(pos - neg) < threshold:  # Neutral\n",
    "            heatmap[i, j] = [0, 0, 0]  # White for neutral, change to [0, 0, 0] for black\n",
    "        elif pos > neg:  # Positive\n",
    "            heatmap[i, j] = [pos, 0, 0]  # Red intensity based on positive value\n",
    "        else:  # Negative\n",
    "            heatmap[i, j] = [0, 0, neg]  # Blue intensity based on negative value\n",
    "\n",
    "# Display the heatmap\n",
    "plt.imshow(heatmap)\n",
    "plt.title(\"Merged Heatmap of Positive and Negative\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f28227d-d702-45ff-82eb-ce6cb4808856",
   "metadata": {},
   "outputs": [],
   "source": [
    "trueNegData = []\n",
    "truePosData = []\n",
    "\n",
    "# Function to fetch data for each image index\n",
    "def fetch_data_for_images(image_indices):\n",
    "    data_array = []\n",
    "    for index in image_indices:\n",
    "        data = explainable_net.get_node_input(layer_name, node_index, index)\n",
    "        data_array.append(data)\n",
    "    return data_array\n",
    "\n",
    "# Fetching data for trueNeg and truePos\n",
    "trueNegData = fetch_data_for_images(trueNeg)\n",
    "truePosData = fetch_data_for_images(truePos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2082a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# Assuming trueNegData and truePosData are lists of numpy arrays\n",
    "trueNegData_np = np.array(trueNegData)\n",
    "truePosData_np = np.array(truePosData)\n",
    "\n",
    "# Convert numpy arrays into PyTorch tensors\n",
    "trueNegData_tensor = torch.tensor(trueNegData_np, dtype=torch.float32)\n",
    "truePosData_tensor = torch.tensor(truePosData_np, dtype=torch.float32)\n",
    "\n",
    "# Create labels for the datasets\n",
    "labels1 = torch.zeros(len(trueNegData), dtype=torch.float32)  # Labels for trueNegData\n",
    "labels2 = torch.ones(len(truePosData), dtype=torch.float32)   # Labels for truePosData\n",
    "\n",
    "# Combine datasets and labels\n",
    "combined_dataset = torch.cat([trueNegData_tensor, truePosData_tensor], dim=0)\n",
    "combined_labels = torch.cat([labels1, labels2], dim=0)\n",
    "\n",
    "# Create a TensorDataset and DataLoader\n",
    "dataset = TensorDataset(combined_dataset, combined_labels)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Initialize model, optimizer, and loss function here\n",
    "# Assuming `model` and `device` are defined elsewhere\n",
    "\n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 10  # Adjust as needed\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model.forward_with_explainable(inputs).squeeze()\n",
    "\n",
    "        # Ensure labels match the output shape\n",
    "        labels = labels.view_as(outputs)\n",
    "\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77448f10-db72-4377-9d5a-69d943a48901",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = all_images[12].to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Forward pass to get both outputs\n",
    "with torch.no_grad():\n",
    "    regular_output, explainable_output = model(inputs, return_explainable=True)\n",
    "\n",
    "print(\"Regular Output:\", regular_output)\n",
    "print(\"Explainable Output:\", explainable_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
